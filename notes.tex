% Created 2017-09-03 Sun 20:09
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Luis Quintanilla}
\date{\today}
\title{Deep Learning A-Z: Hands-On AI}
\hypersetup{
 pdfauthor={Luis Quintanilla},
 pdftitle={Deep Learning A-Z: Hands-On AI},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 24.5.1 (Org mode 9.0.9)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\subsection{Section 1: Welcome to the Course}
\label{sec:org7693b2d}

8/21/2017

\subsubsection{What is Deep Learning}
\label{sec:orgf24582f}

\begin{itemize}
\item Geoffrey Hinton - Godfather of DL. Check out youtube videos.
\item Artificial Neural Nets
\begin{itemize}
\item Made up of nodes
\begin{itemize}
\item Input Nodes - Input Layer (known values)
\item Output Nodes - Output Layer (values to predict)
\item Hidden Layer - Between input and output layer.
\end{itemize}
\item Input -> Hidden -> Output
\item Deep Learning comes in when there are multiple hidden layers.
\end{itemize}
\end{itemize}

\subsubsection{Installing Python}
\label{sec:orgbd8c72f}

\begin{itemize}
\item Install Anaconda Python (version 3.5)
\item Launch Spyder through Anaconda Navigator
\end{itemize}

\subsubsection{Getting The Dataset}
\label{sec:org6abb65b}

\url{http://superdatascience.com/deep-learning}

\begin{itemize}
\item Download Deep Learning A-Z Folder Template
\item Download the dataset for each part of the course
\item Look into Additional Reading mentioned throughout course
\end{itemize}

\subsection{Section 2: Artificial Neural Networks}
\label{sec:org6fd0ef0}

\section{Part 1}
\label{sec:org3e2cd51}

\subsection{Section 3: ANN Intuition}
\label{sec:org4e7f6bf}

\subsubsection{Plan of Attack}
\label{sec:orgf3b5624}
\begin{itemize}
\item What we will learn in this section:
\begin{itemize}
\item The Neuron
\item The Activation Function
\item How do neural networks work? (example)
\item How do neural networks learn?
\item Gradient Descent
\item Stochastic Gradient Descent
\item Backprogation
\end{itemize}
\end{itemize}

\subsubsection{The Neuron}
\label{sec:orgec9f4a2}

\begin{itemize}
\item Basic building block of Artificial Neural Networks
\item Neurons by themselves are useless. With lots of neurons, they work together.
\item Dendrites are receivers of the signal and Axon is transmitter.
\item Synapses - Term for connector that passes signal.

\item Recreating on machine
\begin{itemize}
\item Neuron (node)
\begin{itemize}
\item Receives input (from input layer or other hidden layer neurons)
\item Generates output (to output layer or other hidden layer neurons)
\item Connected via synapses
\end{itemize}
\item Input Values
\begin{itemize}
\item Independent variables
\item All for one single observation (one row in your database)
\item Need to be standadized (mean 0; variance: 1) or normalized (get values between 0 and 1)
\begin{itemize}
\item Easier for neural network to process
\end{itemize}
\end{itemize}
\item Output Values
\begin{itemize}
\item Continous, Binary, Categorical (several output values)
\item All for one single observation
\end{itemize}
\item Synapses
\begin{itemize}
\item All assigned weights.
\item Weights is how the neural net works learn. By adjusting weights, neural net decides which signal gets passed along or not
\item Signals are sent into the neuron where they are added (weighted sum) of all input values and applies an activation function
\item Decision is made based on function whether signal will pass on or not
\end{itemize}
\end{itemize}
\end{itemize}

\subsubsection{The Activation Function}
\label{sec:orge54470d}

\begin{itemize}
\item Threshold Function
\begin{itemize}
\item 1 if x >= 0 | 0 if x < 0 (yes or no)
\end{itemize}
\item Sigmoid Function
\begin{itemize}
\item \(\frac{1}{1 + e^{-x}}\) (used in logistic regression)
\item Smooth gradual progression
\item Very useful in output layer
\end{itemize}
\item Rectifier Function
\begin{itemize}
\item \(max(x,0)\)
\item One of most used
\end{itemize}
\item Hyperbolic Tangent (tahn)
\begin{itemize}
\item \(\frac{1 - e^{-2x}}{ 1 + e^{-2x}}\)
\item Can go below 0
\end{itemize}

\item Exercise 1
\begin{itemize}
\item Assuming DV is binary (y = 0 or 1)
\begin{itemize}
\item Threshold Function
\item Sigmoid Function (probability of y being 1 or not)
\end{itemize}
\end{itemize}
\item Exercise 2
\begin{itemize}
\item In hidden layer apply rectifier
\item Output layer applies sigmoid function
\end{itemize}
\end{itemize}


8/22/2017

\subsubsection{How do Neural Networks work?}
\label{sec:orgcaafff8}

Example: Property Valuations

\begin{itemize}
\item Input Layer
\begin{itemize}
\item Area
\item Bedrooms
\item Distance to city
\item Age
\end{itemize}
\item Output Layer
\begin{itemize}
\item Price
\end{itemize}
\item Hidden Layer
\begin{itemize}
\item Connected with input layer
\item Some weights will have 0 value and others will not
\item Allows you to increase the flexibility or your neural network and look for specific things
\end{itemize}
\end{itemize}

\subsubsection{How do Neural Networks learn?}
\label{sec:orga12f016}

Not rule-based. Neural network deciphers answer by itself given the inputs and a structure

\begin{itemize}
\item Single Layer Feedforward Neural Network (perceptron)
\begin{itemize}
\item Input: \(x\)
\item Output:  \(\hat{y}\)
\item Cost function: C = \(\frac {1}{2}(\hat{y}-y)^2\) (most common)
\begin{itemize}
\item What is the error that you have in your prediction
\item Goal is to minimize the cost function
\end{itemize}
\item Information is fed back through the network and weights are updated
\item Process is repeated with same observation until it converges
\item One epoch is when we go through a whole dataset and train our neural net on all observations
\item Cost function for multiple rows is \(C=\sum\frac{1}{2}(\hat{y} - y) ^ 2\). All rows share the same weights
\item Process of feedback is called backpropagation
\end{itemize}
\end{itemize}

\subsubsection{Gradient Descent}
\label{sec:org03c600c}

\begin{itemize}
\item How do we reduce the cost function?
\item The brute force method is to try out infinitely many weights to see which one is best. This is extremely inefficient due to the curse of dimensionality
\item Gradient Descent starts somewhere and look at the slope is positive or negative (down or uphill)
\item Then a "step" is taken and slope is recalculated to find out the next step using the new weight as the starting position
\end{itemize}

\subsubsection{Stochastic Gradient Descent}
\label{sec:org7b237e5}

\begin{itemize}
\item Gradient Descent is a way to help solve optimization problem of minimizing cost function
\item However, Gradient Descent requires that the cost function is convex (parabola)
\item If the cost function is not convex, Gradient Descent finds local minimum instead of global
\item Stochastic Gradient Descent does not require a convex cost function
\item With Stochastic Gradient Descent, the weights are updated on an observation basis as opposed to the aggregate after processing all observations
\item Stochastic Gradient Descent allows you to avoid getting stuck at local minima
\item Stochastic Gradient Descent has greater fluctuations
\item Stochastic Gradient Descent is faster because all weights are not updated at the same time and data is not in memory
\item Gradient Descent is deterministic algorithm and Stochastic Gradient Descent is Stochastic
\item MiniBatch Gradient Descent method is a combination of both Gradient Desent and Stochastic Gradient Descent
\end{itemize}

\subsubsection{Backpropagation}
\label{sec:org5fbdb08}


\subsection{Section 4: Building an ANN}
\label{sec:org8590947}

\subsection{Section 5: Homework Challenge - Should we say goodbye to that customer?}
\label{sec:orgd4ac517}

\subsection{Section 6: Evaluation, Improving and Tuning the ANN}
\label{sec:orgc25a953}

\subsection{Section 7: Homework Challenge - Put me one step down on the podium}
\label{sec:org0f4123f}

\subsection{Section 8: Convolutional Neural Networks}
\label{sec:org2a47bda}
\end{document}
